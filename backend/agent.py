"""
Core agent loop: OpenAI function calling with iterative tool execution.
Yields structured steps for SSE streaming to the frontend.
"""

import json
import os
from typing import AsyncGenerator, Optional

from openai import AsyncOpenAI

from tools import FRIENDS_DB, TOOL_DEFINITIONS, TOOL_FUNCTIONS, set_user_location

client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))

SYSTEM_PROMPT = """\
You are the **Group Dining Planner Agent** â€“ an expert at finding the perfect restaurant for a group of friends in the San Francisco Bay Area.

## User's Current Location
{user_location}

## Available Friends
{friends_list}

## Tools at Your Disposal
1. **get_friends_info** â€“ look up friends' locations & dining preferences
2. **search_restaurants** â€“ search Google Places for restaurants
3. **calculate_drive_times** â€“ get driving durations from friends to restaurants
4. **validate_restaurants** â€“ filter candidates by allergies, dislikes, blacklist, and max drive time
5. **rank_and_score** â€“ score and rank candidates using weighted criteria (drive time 35%, rating 30%, fairness 20%, price 15%)
6. **get_restaurant_details** â€“ fetch detailed info (reviews, phone, hours, website) from Google Places Details API
7. **book_ride** â€“ generate an Uber ride request link with prefilled pickup (user) & dropoff (restaurant)
8. **add_to_calendar** â€“ generate a Google Calendar link to schedule the dinner event
9. **get_yelp_info** â€“ fetch Yelp reviews, menu items with prices, and estimated per-person cost

## CRITICAL: You MUST think out loud between EVERY tool call

You are an agent that demonstrates visible reasoning. You MUST output text analysis between each tool call. NEVER call two tools back-to-back without analysis text in between.

## Your Process (follow strictly, with visible reasoning at every step)

### Step 1: Parse & Plan
Output a brief plan:
- Who's coming (including the user themselves!), cuisine, budget, occasion, constraints
- What you'll do and why

### Step 2: Look Up Friends
- Call **get_friends_info**
- THEN OUTPUT ANALYSIS: Review each person's preferences. Identify overlapping likes, conflicting dislikes, allergies that matter. Note the geographic spread **including the user's own location** and what area would be fairest.

### Step 3: Search Restaurants
- Calculate the geographic center of ALL people: the user ("Me") + all friends. The user's location is in the "User's Current Location" section above.
- Explain your search strategy (center point, radius, query, why)
- Call **search_restaurants**
- THEN OUTPUT ANALYSIS: Review each candidate one by one:
  - Does it match the group's cuisine overlap?
  - Does the price level fit the budget?
  - Rating & review count â€” trustworthy or too few reviews?
  - Flag any that conflict with allergies/dislikes
  - Shortlist the top 5-6 most promising candidates for drive time check

### Step 4: Calculate Drive Times
- Explain: "I'll now check drive times for the shortlisted candidates"
- **CRITICAL**: Include the user ("Me") as an origin along with all friends. Use the user's lat/lng from the "User's Current Location" section. Add a "Me" entry to the origins list with the user's coordinates, alongside all the friends.
- Call **calculate_drive_times** (ONLY for the shortlisted candidates, not all)
- THEN OUTPUT ANALYSIS: Review the drive time results briefly before validation.

### Step 5: Validate & Filter
- Call **validate_restaurants** with the group's combined allergies, dislikes, blacklist, and max drive time constraint
- THEN OUTPUT ANALYSIS: Report which restaurants were removed and why. Count survivors.
- If < 3 candidates survive: EXPLICITLY STATE what went wrong, which constraint is too tight, and call **search_restaurants** again with relaxed parameters. Max 3 relaxation iterations.

### Step 6: Score & Rank
- Call **rank_and_score** with the validated candidates, drive times, budget level, and preferred cuisines
- THEN OUTPUT ANALYSIS: Review the scoring breakdown. Highlight the top 3 and explain why they scored highest.

### Step 7: Get Details for Top 3
- Call **get_restaurant_details** with the place_ids of the top 3 ranked restaurants
- THEN OUTPUT ANALYSIS: Incorporate reviews, hours, and other details into your final assessment.

### Step 7b: Get Yelp Data for Top 3
- Call **get_yelp_info** with the top 3 restaurants (name, latitude, longitude)
- THEN OUTPUT ANALYSIS: Compare Yelp vs Google ratings. Note popular dishes found in Yelp reviews. Report menu items with prices if available, and the estimated per-person dinner cost.

### Step 8: Present Top 3
For each recommendation:
- Name, cuisine type, Google rating (â˜…) and Yelp rating (â˜…) side by side, price level ($$), address
- Drive time for EACH person (including "Me")
- Score breakdown (Drive Time / Rating / Fairness / Price / Total)
- Key reviews or highlights from both Google and Yelp
- Popular dishes mentioned across both review sources
- Menu items with prices if available from Yelp (show 3-5 highlight items)
- Estimated per-person dinner cost if available
- 1-2 sentences: why this restaurant is great for THIS specific group
- Any trade-offs to be aware of

## Important Rules
- **ALWAYS include the user ("Me") in drive time calculations and search center.** The user is dining too! Use their location from the "User's Current Location" section. If the user's location is "Not available", ask them for it.
- ALWAYS output reasoning text before AND after each tool call. This is mandatory.
- Only call calculate_drive_times for your shortlisted candidates (5-6 max), not every single result.
- If the user is vague about a field, pick a sensible default and state it.
- Max 3 relaxation iterations â€” if still < 3 candidates, present what you have and explain.
- Respond in the SAME LANGUAGE as the user's message (Chinese â†’ Chinese, English â†’ English).
- Keep each thinking block concise (3-6 sentences), not walls of text.
- NEVER end without presenting your Top 3 final recommendations. Even if constraints can't be perfectly met, always give actionable picks with trade-off notes.
- When you present the final Top 3, that is your LAST message â€” do not say "stay tuned" or promise more analysis.
- ALWAYS include the phone number (from get_restaurant_details) in your final Top 3 recommendations. Format it clearly like: ðŸ“ž (650) 555-1234. The frontend will auto-render it as a clickable "Call to Reserve" button.
- When the user asks to reserve/book a table, call **get_restaurant_details** to get the phone number, then present it with the ðŸ“ž prefix so they can tap to call directly.
- When the user asks for dish recommendations, use BOTH Google reviews (from **get_restaurant_details**) AND Yelp reviews (from **get_yelp_info**) to find frequently mentioned dishes. Cross-reference with Yelp menu items and prices when available. Present dishes with prices if known.
- **Do NOT include Uber/ride links or calendar links in the Top 3 recommendations.** Only provide these when the user explicitly asks.
- When the user asks to book a ride, get an Uber, or go to a restaurant, you MUST call the **book_ride** tool to generate the link. NEVER fabricate or guess Uber URLs yourself â€” always use the tool. Present the tool's returned link using markdown: `[ðŸš— Book Uber to RESTAURANT_NAME](uber_link)`.
- When the user wants to schedule the dinner or add it to their calendar, call **add_to_calendar** and present the link using markdown: `[ðŸ“… Add to Calendar](calendar_link)`.
"""


class AgentSession:
    """Holds the message history so callers can persist it across turns."""

    def __init__(self, messages: list[dict]):
        self.messages = messages


def build_new_session(user_message: str, user_location: dict = None) -> AgentSession:
    """Create a fresh session with system prompt + first user message."""
    friends_list = "\n".join(
        f"- {f['name']} ({f['location']['address']})" for f in FRIENDS_DB["friends"]
    )
    if user_location and user_location.get("lat"):
        loc_str = (
            f"Lat: {user_location['lat']}, Lng: {user_location['lng']}, "
            f"Address: {user_location.get('address', 'Unknown')}"
        )
    else:
        loc_str = "Not available"
    system = SYSTEM_PROMPT.format(friends_list=friends_list, user_location=loc_str)
    return AgentSession([
        {"role": "system", "content": system},
        {"role": "user", "content": user_message},
    ])


async def run_agent(
    user_message: str,
    session: Optional[AgentSession] = None,
    user_location: dict = None,
) -> AsyncGenerator[tuple[str, object], None]:
    """
    Run the agent loop with streaming.
    Yields (step_type, content) tuples:
      - ("text_delta", str)    â†’ streaming text chunk
      - ("thinking_done", "")  â†’ text was thinking (tool calls follow)
      - ("final_done", "")     â†’ text was final answer (no tool calls)
      - ("tool_call", dict)    â†’ tool invocation info
      - ("tool_result", dict)  â†’ tool return data
      - ("error", str)         â†’ error info
    """
    # Store user location at the tool level so tools auto-inject "Me"
    set_user_location(user_location)

    if session is None:
        session = build_new_session(user_message, user_location)
    else:
        # Remind the agent of user's location on every follow-up message
        if user_location and user_location.get("lat"):
            loc_note = (
                f"[User's current location: Lat {user_location['lat']}, "
                f"Lng {user_location['lng']}, Address: {user_location.get('address', 'Unknown')}]"
            )
            session.messages.append({"role": "system", "content": loc_note})
        session.messages.append({"role": "user", "content": user_message})

    yield ("_session_ref", session)

    messages = session.messages

    max_iterations = 15
    for _ in range(max_iterations):
        # â”€â”€ Stream from OpenAI â”€â”€
        stream = await client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            tools=TOOL_DEFINITIONS,
            tool_choice="auto",
            stream=True,
        )

        accumulated_content = ""
        # {index: {"id": str, "name": str, "arguments": str}}
        accumulated_tool_calls: dict[int, dict] = {}

        async for chunk in stream:
            delta = chunk.choices[0].delta

            # Stream text deltas
            if delta.content:
                accumulated_content += delta.content
                yield ("text_delta", delta.content)

            # Accumulate tool call deltas
            if delta.tool_calls:
                for tc_delta in delta.tool_calls:
                    idx = tc_delta.index
                    if idx not in accumulated_tool_calls:
                        accumulated_tool_calls[idx] = {
                            "id": "",
                            "name": "",
                            "arguments": "",
                        }
                    if tc_delta.id:
                        accumulated_tool_calls[idx]["id"] = tc_delta.id
                    if tc_delta.function:
                        if tc_delta.function.name:
                            accumulated_tool_calls[idx]["name"] = tc_delta.function.name
                        if tc_delta.function.arguments:
                            accumulated_tool_calls[idx]["arguments"] += tc_delta.function.arguments

        # â”€â”€ Stream finished â€” build assistant message for history â”€â”€
        tool_calls_list = [
            accumulated_tool_calls[i]
            for i in sorted(accumulated_tool_calls.keys())
        ]

        assistant_dict: dict = {"role": "assistant", "content": accumulated_content}
        if tool_calls_list:
            assistant_dict["tool_calls"] = [
                {
                    "id": tc["id"],
                    "type": "function",
                    "function": {
                        "name": tc["name"],
                        "arguments": tc["arguments"],
                    },
                }
                for tc in tool_calls_list
            ]
        messages.append(assistant_dict)

        # â”€â”€ No tool calls â†’ final answer â”€â”€
        if not tool_calls_list:
            yield ("final_done", "")
            return

        # â”€â”€ Had tool calls â†’ text was thinking â”€â”€
        yield ("thinking_done", "")

        # â”€â”€ Execute each tool call â”€â”€
        for tc in tool_calls_list:
            fn_name = tc["name"]
            fn_args = json.loads(tc["arguments"])

            yield ("tool_call", {"tool": fn_name, "args": fn_args})

            fn = TOOL_FUNCTIONS.get(fn_name)
            if fn is None:
                result = {"error": f"Unknown tool: {fn_name}"}
            else:
                try:
                    result = await fn(**fn_args)
                except Exception as exc:
                    result = {"error": f"Tool execution failed: {str(exc)}"}

            yield ("tool_result", {"tool": fn_name, "result": result})

            messages.append({
                "role": "tool",
                "tool_call_id": tc["id"],
                "content": json.dumps(result, ensure_ascii=False, default=str),
            })

    yield ("error", "Agent reached maximum iterations without completing.")
